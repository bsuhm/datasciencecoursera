---
title: "HAR Data Analysis"
author: "Bernhard Suhm"
date: "Thursday, May 14, 2015"
output: html_document
---


Preparation
-----------

(Note to myself for future reference) This work will access the caret package, kernlab (for the spam dataset), and Hmisc. Remember you need two steps, install.packages, followed by library.

We begin by loading the training data set and partitioning it into 60% training and 40% for and cross validation and testing. 
```{r, echo=FALSE}
library(caret)
HAR <- read.csv("pml-training.csv", na.string=c("NA","#DIV/0!"))
clean <- HAR[,8:160]
inTrain <- createDataPartition(clean$classe,p=0.6,list=FALSE)
train_har <-HAR[inTrain,]
heldout_har <- HAR[-inTrain,]
```

Next we need to select the variables we'll actually use in the model. First step, let's remove all variables are near zero and thus will have little predictive power. Once that's done, we can split the 40% into a cross validation and test set consisting of 20% of the original data, with the 27 near zero variables eliminated. We'll use the cross validation set to explore different models, and the test set (only once, with the best performing model on the cross validation set) to get a good estimate of accuracy on completely new (unseen) data.
```{r, echo=TRUE}
nzv_har <- nearZeroVar(train_har,saveMetrics=TRUE)
sum(nzv_har$nzv==TRUE)
train<-train_har[!nzv_har$nzv]
heldout<-heldout_har[!nzv_har$nzv]
testSplit <- createDataPartition(heldout$classe,p=0.5,list=FALSE)
test<-heldout[testSplit,]
crossVal<-heldout[-testSplit,]
```

Then I explored which variables separated the outcome variables visually in plots, and this only among the non near zero ones. Visually, the following variables had some predictive power:
* All three belt (roll, pitch, yaw), max_roll_belt, max_pitch_belt
* forearm: pitch and roll
though then I realized that all but the 52 "original" data variables were primarily NA values, hence not suitable for model training. With "original" I mean the ones that represent sensor measurements, as opposed to those derived from measurements by applying a window over the original data - thanks to Edward Drake for explaining this so nicely in the discussion forum! I suspect something is wrong with the data offered on the website, i.e., there was a bug when they calculated those derived measures. So for the model training phase, we'll just be using the 52 variables.


Model Training
--------------

First, I wanted to train a linear model, even though my intuition suggested that's probably not a good fit for this problem. The caret train function had trouble using all variables, but then I used the gbm package and succeeded training a model.
```{r, echo=FALSE}
library(gbm)
fol52 <- formula(classe ~ roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x +gyros_forearm_y +  gyros_forearm_z + accel_forearm_x + accel_forearm_y + accel_forearm_z + magnet_forearm_x + magnet_forearm_y + magnet_forearm_z + roll_belt + pitch_belt + yaw_belt + total_accel_belt + gyros_belt_x +gyros_belt_y +  gyros_belt_z + accel_belt_x + accel_belt_y + accel_belt_z + magnet_belt_x + magnet_belt_y + magnet_belt_z + roll_dumbbell + pitch_dumbbell + yaw_dumbbell + total_accel_dumbbell + gyros_dumbbell_x +gyros_dumbbell_y +  gyros_dumbbell_z + accel_dumbbell_x + accel_dumbbell_y + accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z+roll_arm + pitch_arm + yaw_arm + total_accel_arm + gyros_arm_x +gyros_arm_y +  gyros_arm_z + accel_arm_x + accel_arm_y + accel_arm_z + magnet_arm_x + magnet_arm_y + magnet_arm_z)
m_gbm <- gbm(fol52,data=train)
p_gbm<- predict(m_gbm,crossVal,n.trees=100,type="response")
pCV_gbm<-apply(p_gbm,1,which.max) # need to pick out best choice since predict returns a vector of pseudo probs
accCV_gbm<-sum(pCV_gbm==as.numeric(crossVal$classe))/nrow(crossVal)
```

The dominant variables in this model by far are roll_belt and pitch_forearm, with some influence by magnet_dumbbell_y, accel_forearm_x, and roll_dumbbell. Only two other variables have non zero influence at all: gyros_dumbbell_y and accel_dumbbel_x. The fact that only 7 out of 52 predictors had a non-zero influence is somewhat disconcerning. Therefore not surprisingly, it's accuracy is rather low, 50% on the cross validation set. We are even less surprised when we plot the two dominant variables against the classes, we see some nice separation - but clearly not with hyperplanes (linear functions).

```{r, echo=TRUE}
qplot(roll_belt,pitch_forearm,color=classe,data=train)
```

Next I tried LDA, more for practice than seriously expecting it to be the best performer, since I already mentioned earlier the plots aren't separable by hyperplanes upon visual inspection of some clearly important predictor variables.
```{r, echo=FALSE}
m_lda<-train(fol52,method="lda",data=train)
p_lda <- predict(m_lda,crossVal,type="raw")
acc_lda <- sum(p_lda==crossVal$classe)/nrow(crossVal)
```
This model gets 69.9% accuracy on cross validation. Reducing the variables with PCA (using the preprocessing step that's built into the caret train package) actually lowers the accuracy to 53.3%.

Next, I trained a random forest on those same 52 variables, using the randomForest package that Bill Howe from University of Washington uses in his Data Science class. It runs a lot faster than the corresponding variant of train in the caret package!

```{r, echo=FALSE}
library(randomForest)
m_rf <- randomForest(fol52,data=train)
p_rf <- predict(m_rf,crossVal,type="class")
acc_rf <- sum(p_rf==crossVal$classe)/nrow(crossVal)
```
That model got a surprising 99.3% accuracy on the cross validation set. Let's see which variables had the greatest influence - roll_best, yaw_belt, pitch_forearm, and plot the top two against the categories like above:
```{r, echo=TRUE}
rf_imp <- importance(m_rf)
sort(rf_imp[,1],decreasing=TRUE)
qplot(roll_belt,yaw_belt,color=classe,data=train)
```

This high an accuracy was surprising to me (and many co-students on the discussion forums), but the HAR paper and website mentions an accuracy of >99%.


Model Evaluation
----------------

The accuracy of that random forest on the held-out test set is 99.4%. So this should be a good predictor of the accuracy of this model on new (unseen) data. Now it's time to evaluate the model on the 20 test cases. Turns out to be 20 out of 20 cases. 



